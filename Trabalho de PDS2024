# Importar pacotes necessários
import pandas as pd
import numpy as np
from scipy.signal import butter, ellip, filtfilt
from numpy.fft import fft, fftfreq
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import f1_score
import pydotplus

# Carregar o arquivo localmente
file_path = 'DogMoveData.csv'  # Caminho para o arquivo já baixado

# Ler o arquivo CSV
df = pd.read_csv(file_path)

# Verificar se as colunas 'Behavior_1', 'Behavior_2', 'Behavior_3' estão presentes
if all(col in df.columns for col in ['Behavior_1', 'Behavior_2', 'Behavior_3']):
    # Combinar as colunas de comportamento em uma única coluna de comportamento consolidado
    def consolidate_behavior(row):
        behaviors = row[['Behavior_1', 'Behavior_2', 'Behavior_3']].dropna().unique()
        if len(behaviors) > 0:
            return behaviors[0]  # Pega o primeiro comportamento não nulo encontrado
        return '<undefined>'

    df['Behavior'] = df.apply(consolidate_behavior, axis=1)
else:
    raise ValueError("Colunas 'Behavior_1', 'Behavior_2' e 'Behavior_3' não foram encontradas no arquivo CSV.")

# Filtrar apenas comportamentos válidos
df = df[df['Behavior'].isin(['Galloping', 'Standing', 'Walking'])]

# Converter a coluna 'Behavior' para valores numéricos
behavior_mapping = {'Galloping': 1, 'Standing': 2, 'Walking': 3}
df['Behavior'] = df['Behavior'].map(behavior_mapping)

# Calcular normas euclidianas do acelerômetro e giroscópio
df['ANorm'] = np.linalg.norm(df[['ANeck_x', 'ANeck_y', 'ANeck_z']], axis=1)
df['GNorm'] = np.linalg.norm(df[['GNeck_x', 'GNeck_y', 'GNeck_z']], axis=1)

# Função para aplicação de filtros
def apply_filter(data, filter_type='low', cutoff=10, fs=50, order=5):
    b, a = (butter(order, cutoff / (0.5 * fs), btype=filter_type) 
            if filter_type == 'low' else ellip(order, 0.1, 40, cutoff / (0.5 * fs), btype='high'))
    return filtfilt(b, a, data)

# Aplicar filtros aos sinais
dog_ids = df['DogID'].unique()
for dog_id in dog_ids:
    subset = df[df['DogID'] == dog_id]
    df.loc[subset.index, 'FilteredANorm'] = apply_filter(apply_filter(subset['ANorm'], 'high'), 'low')
    df.loc[subset.index, 'FilteredGNorm'] = apply_filter(apply_filter(subset['GNorm'], 'high'), 'low')

# Funções para extração de características
def energy(sig): return np.sum(sig ** 2) * 0.01
def peak(sig): return fftfreq(len(sig))[np.argmax(np.abs(fft(sig)))]
def cross_mean(sig): return np.sum(np.diff(np.sign(sig - np.mean(sig))) != 0)

# Função de janelação
def windowing(data, window_size=200, overlap=0.5, window_type='hann'):
    step = int(window_size * (1 - overlap))
    window = np.hanning(window_size) if window_type == 'hann' else np.ones(window_size)
    return [data[i:i + window_size] * window for i in range(0, len(data) - window_size + 1, step)]

# Processamento dos dados por janela
window_size = 200
id, ea, eg, pa, pg, cma, cmg = [], [], [], [], [], [], []
for dog_id in dog_ids:
    dog = df[df['DogID'] == dog_id]
    a_win = windowing(dog['FilteredANorm'].values, window_size)
    g_win = windowing(dog['FilteredGNorm'].values, window_size)

    for sig_w_a, sig_w_g in zip(a_win, g_win):
        id.append(dog_id)
        ea.append(energy(sig_w_a))
        eg.append(energy(sig_w_g))
        pa.append(peak(sig_w_a))
        pg.append(peak(sig_w_g))
        cma.append(cross_mean(sig_w_a))
        cmg.append(cross_mean(sig_w_g))

# Criar DataFrame consolidado
df_consolidated = pd.DataFrame({
    'DogID': id,
    'Energy_A': ea,
    'Energy_G': eg,
    'Peak_A': pa,
    'Peak_G': pg,
    'Cross_Mean_A': cma,
    'Cross_Mean_G': cmg
})

# Converter todas as colunas de entrada para float
df_consolidated = df_consolidated.astype(float)

# Adicionar colunas de normas calculadas e sinais filtrados
df_consolidated['ANorm'] = df['ANorm'].iloc[:len(df_consolidated)].values
df_consolidated['GNorm'] = df['GNorm'].iloc[:len(df_consolidated)].values
df_consolidated['FilteredANorm'] = df['FilteredANorm'].iloc[:len(df_consolidated)].values
df_consolidated['FilteredGNorm'] = df['FilteredGNorm'].iloc[:len(df_consolidated)].values

# Adicionar comportamento consolidado ao DataFrame
df_consolidated['Behavior'] = df['Behavior'].iloc[:len(df_consolidated)].values

# Classificação usando Decision Tree
X = df_consolidated[['Energy_A', 'Energy_G', 'Peak_A', 'Peak_G', 'Cross_Mean_A', 'Cross_Mean_G']]
y = df_consolidated['Behavior']

# Garantir que X e y sejam numéricos
X = X.astype(float)
y = y.astype(int)

# Treinamento do modelo
clf = DecisionTreeClassifier(max_depth=5)
clf.fit(X, y)
f1 = f1_score(y, clf.predict(X), average='weighted')

# Adicionar escore F1 ao DataFrame consolidado
df_consolidated['F1_Score'] = [f1] * len(df_consolidated)

# Salvar resultados em arquivo CSV
df_consolidated.to_csv('df_consolidated.csv', index=False)

# Exibir as informações principais no console
print("Escore F1 médio: {:.4f}".format(f1))
print(df_consolidated.head())
